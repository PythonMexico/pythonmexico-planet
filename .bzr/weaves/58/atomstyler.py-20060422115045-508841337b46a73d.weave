# bzr weave file v5
i
1 8ea2ee71acec4e6046a12af0f21f90d14ac065fd
n rubys@intertwingly.net-20060422124840-1b2ba6db416d5b9f

i 0
1 57c745b2f71ebc36ea047d7fa0fca4eeb15fd97c
n rubys@intertwingly.net-20060423221402-0892fd4c924a21c6

i 1
1 14bd941ad38b7c1ad32b9d4eefddf64273a33ddb
n rubys@intertwingly.net-20060508144742-cbb3ebf9185a2c8a

i 2
1 07638c7140f7ffbe7d65fd101ab846823a8e879c
n rubys@intertwingly.net-20060509010717-187bcdc16d702723

i 3
1 769e52838d819d2189df0afb75b64184ea16aa6f
n rubys@intertwingly.net-20060512045907-e4e7baa333752135

w
{ 0
. from xml.dom import minidom, Node
. from urlparse import urlparse, urlunparse
. from xml.parsers.expat import ExpatError
{ 1
. from htmlentitydefs import name2codepoint
. import re
}
. 
. # select and apply an xml:base for this entry
. class relativize:
.   def __init__(self, parent):
.     self.score = {}
{ 2
.     self.links = []
}
.     self.collect_and_tally(parent)
.     self.base = self.select_optimal_base()
.     if self.base:
.       if not parent.hasAttribute('xml:base'):
.         self.rebase(parent)
.         parent.setAttribute('xml:base', self.base)
. 
[ 3
.   # collect and tally href and src attributes
] 3
{ 3
.   # collect and tally cite, href and src attributes
}
.   def collect_and_tally(self,parent):
.     uri = None
{ 3
.     if parent.hasAttribute('cite'): uri=parent.getAttribute('cite')
}
.     if parent.hasAttribute('href'): uri=parent.getAttribute('href')
.     if parent.hasAttribute('src'): uri=parent.getAttribute('src')
. 
.     if uri:
.       parts=urlparse(uri)
.       if parts[0].lower() == 'http':
.         parts = (parts[1]+parts[2]).split('/')
{ 2
.         base = None
}
.         for i in range(1,len(parts)):
.           base = tuple(parts[0:i])
.           self.score[base] = self.score.get(base,0) + len(base)
{ 2
.         if base and base not in self.links: self.links.append(base)
}
. 
.     for node in parent.childNodes:
.       if node.nodeType == Node.ELEMENT_NODE:
.         self.collect_and_tally(node)
.     
.   # select the xml:base with the highest score
.   def select_optimal_base(self):
.     if not self.score: return None
{ 2
.     for link in self.links:
.       self.score[link] = 0
}
.     winner = max(self.score.values())
{ 2
.     if not winner: return None
}
.     for key in self.score.keys():
.       if self.score[key] == winner:
.         if winner == len(key): return None
.         return urlunparse(('http', key[0], '/'.join(key[1:]), '', '', '')) + '/'
.     
[ 3
.   # rewrite href and src attributes using this base
] 3
{ 3
.   # rewrite cite, href and src attributes using this base
}
.   def rebase(self,parent):
.     uri = None
{ 3
.     if parent.hasAttribute('cite'): uri=parent.getAttribute('cite')
}
.     if parent.hasAttribute('href'): uri=parent.getAttribute('href')
.     if parent.hasAttribute('src'): uri=parent.getAttribute('src')
.     if uri and uri.startswith(self.base):
.       uri = uri[len(self.base):] or '.'
.       if parent.hasAttribute('href'): uri=parent.setAttribute('href', uri)
.       if parent.hasAttribute('src'): uri=parent.setAttribute('src', uri)
. 
.     for node in parent.childNodes:
.       if node.nodeType == Node.ELEMENT_NODE:
.         self.rebase(node)
. 
. # convert type="html" to type="plain" or type="xhtml" as appropriate
. def retype(parent):
.   for node in parent.childNodes:
.     if node.nodeType == Node.ELEMENT_NODE:
{ 3
. 
}
.       if node.hasAttribute('type') and node.getAttribute('type') == 'html':
[ 4
.         if len(node.childNodes)==1:
{ 1
] 4
{ 4
.         if len(node.childNodes)==0:
.           node.removeAttribute('type')
.         elif len(node.childNodes)==1:
}
. 
.           # replace html entity defs with utf-8
.           chunks=re.split('&(\w+);', node.childNodes[0].nodeValue)
.           for i in range(1,len(chunks),2):
.              if chunks[i] in ['amp', 'lt', 'gt', 'apos', 'quot']:
.                chunks[i] ='&' + chunks[i] +';'
.              elif chunks[i] in name2codepoint:
.                chunks[i]=unichr(name2codepoint[chunks[i]])
.              else:
.                chunks[i]='&' + chunks[i] + ';'
.           text = u"".join(chunks)
. 
}
.           try:
[ 1
.             text = node.childNodes[0].nodeValue
] 1
{ 1
.             # see if the resulting text is a well-formed XML fragment
}
.             div = '<div xmlns="http://www.w3.org/1999/xhtml">%s</div>'
.             data = minidom.parseString((div % text.encode('utf-8')))
. 
.             if text.find('<') < 0:
.               # plain text
.               node.removeAttribute('type')
.               text = data.documentElement.childNodes[0].nodeValue
.               node.childNodes[0].replaceWholeText(text)
. 
.             elif len(text) > 80:
.               # xhtml
.               node.setAttribute('type', 'xhtml')
.               node.removeChild(node.childNodes[0])
.               node.appendChild(data.documentElement)
[ 3
.               if node.nodeName == 'content':
.                 relativize(node.parentNode)
] 3
. 
.           except ExpatError:
.             # leave as html
.             pass
. 
.       else:
.         # recurse
.         retype(node)
{ 3
. 
.   if parent.nodeName == 'entry':
.     relativize(parent)
}
. 
. if __name__ == '__main__':
. 
.   # run styler on each file mention on the command line
.   import sys
.   for feed in sys.argv[1:]:
.     doc = minidom.parse(feed)
.     doc.normalize()
.     retype(doc.documentElement)
.     open(feed,'w').write(doc.toxml('utf-8'))
}
W
